{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNes8ON/V6ypJxRwnSTy3oB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robbarto2/GenAI-Foundations/blob/main/GTP2_Chatbot_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "AQTpEpChO4B-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "9q6obI8zO3z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Notes:*\n",
        "\n",
        "**model_name = \"gpt2\"**\n",
        "This sets the model name to \"gpt2\", which refers to a specific pre-trained model available in the Hugging Face transformers library. GPT-2 (Generative Pre-trained Transformer 2). By assigning the model name we specify which model will be loaded in the following code.\n",
        "\n",
        "**model = GPT2LMHeadModel.from_pretrained(model_name)**\n",
        "This loads the GPT2LMHeadModel class from the Hugging Face library.\n",
        "\n",
        "Note: **GPT2LMHeadModel** is a class that represents the specific architecture used for GPT-2, which is designed for language modeling tasks (e.g., generating text). The \"LMHead\" refers to the \"language modeling head,\" meaning this model is ready for tasks like text generation\n"
      ],
      "metadata": {
        "id": "dUg6uxwDGP_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model in evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "sT5rbAtUO3kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Notes:*\n",
        "\n",
        "**model.eval():** When you are using a PyTorch model like the GPT-2 model in this case, it can operate in two modes:\n",
        "\n",
        "*Training Mode (default):* Used when you are training the model (updating its weights). *Evaluation Mode: Used when you are evaluating or using the model for inference (i.e., generating predictions without changing the modelâ€™s weights).*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**wte** stands for \"word token embeddings.\" GPT-2 uses an embedding layer that maps each of the 50,257 possible tokens (words or subwords in the tokenizer's vocabulary) to a 768-dimensional vector. This allows the model to represent each token as a fixed-size numerical vector.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**wpe** stands for \"word position embeddings.\" This layer encodes the position of each token in the sequence (i.e., its place in the input sentence). GPT-2 can process sequences up to 1,024 tokens long, and this layer provides a 768-dimensional representation for each position.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**ModuleList** is the list of Transformer blocks (or layers), which are repeated 12 times in GPT-2. Each block is responsible for processing the input, capturing relationships between tokens, and passing information to the next layer."
      ],
      "metadata": {
        "id": "UJ7mT28lHmJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input prompt\n",
        "prompt = \"What's a great way to get in shape?\""
      ],
      "metadata": {
        "id": "vpRyujTbO3Wa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Generate attention mask\n",
        "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "# Generate text continuation with sampling\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.1,  # Lower temperature for less random text\n",
        "        top_p=0.9,        # Nucleus sampling\n",
        "        top_k=50,         # Restrict to top 50 tokens\n",
        "        do_sample=True    # Enable sampling for diverse output\n",
        "    )"
      ],
      "metadata": {
        "id": "CfGjd7eRPAdS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note:*\n",
        "\n",
        "**attention_mask = torch.ones(input_ids.shape, dtype=torch.long)**\n",
        "This line creates an attention mask for the input sequence. The attention mask is used to tell the model which tokens in the input are real tokens and which are padding tokens (if any)."
      ],
      "metadata": {
        "id": "bqSs7UNaIv5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the result\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "9eq0yLVwOhSh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}