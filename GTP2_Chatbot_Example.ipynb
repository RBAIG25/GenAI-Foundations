{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO4oZsNVUZuZbMJA3bTZUha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robbarto2/GenAI-Foundations/blob/main/GTP2_Chatbot_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "AQTpEpChO4B-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q6obI8zO3z2",
        "outputId": "bab97b24-627b-44a9-de5c-02f8c358f93e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Notes:*\n",
        "\n",
        "**model_name = \"gpt2\"**\n",
        "This sets the model name to \"gpt2\", which refers to a specific pre-trained model available in the Hugging Face transformers library. GPT-2 (Generative Pre-trained Transformer 2). By assigning the model name we specify which model will be loaded in the following code.\n",
        "\n",
        "**model = GPT2LMHeadModel.from_pretrained(model_name)**\n",
        "This loads the GPT2LMHeadModel class from the Hugging Face library.\n",
        "\n",
        "Note: **GPT2LMHeadModel** is a class that represents the specific architecture used for GPT-2, which is designed for language modeling tasks (e.g., generating text). The \"LMHead\" refers to the \"language modeling head,\" meaning this model is ready for tasks like text generation\n"
      ],
      "metadata": {
        "id": "dUg6uxwDGP_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model in evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT5rbAtUO3kU",
        "outputId": "6f633014-cb3e-4e59-8567-026950191f2b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Notes:*\n",
        "\n",
        "**wte** stands for \"word token embeddings.\" GPT-2 uses an embedding layer that maps each of the 50,257 possible tokens (words or subwords in the tokenizer's vocabulary) to a 768-dimensional vector. This allows the model to represent each token as a fixed-size numerical vector.\n",
        "\n",
        "**wpe** stands for \"word position embeddings.\" This layer encodes the position of each token in the sequence (i.e., its place in the input sentence). GPT-2 can process sequences up to 1,024 tokens long, and this layer provides a 768-dimensional representation for each position.\n",
        "\n",
        "**ModuleList** is the list of Transformer blocks (or layers), which are repeated 12 times in GPT-2. Each block is responsible for processing the input, capturing relationships between tokens, and passing information to the next layer."
      ],
      "metadata": {
        "id": "UJ7mT28lHmJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input prompt\n",
        "prompt = \"Can you suggest any ways to get promoted at work?\""
      ],
      "metadata": {
        "id": "vpRyujTbO3Wa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Generate attention mask\n",
        "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "# Generate text continuation with sampling\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.7,  # Lower temperature for less random text\n",
        "        top_p=0.9,        # Nucleus sampling\n",
        "        top_k=50,         # Restrict to top 50 tokens\n",
        "        do_sample=True    # Enable sampling for diverse output\n",
        "    )"
      ],
      "metadata": {
        "id": "CfGjd7eRPAdS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note:*\n",
        "\n",
        "**attention_mask = torch.ones(input_ids.shape, dtype=torch.long)**\n",
        "This line creates an attention mask for the input sequence. The attention mask is used to tell the model which tokens in the input are real tokens and which are padding tokens (if any)."
      ],
      "metadata": {
        "id": "bqSs7UNaIv5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the result\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eq0yLVwOhSh",
        "outputId": "b6799412-4224-43f6-f44a-ecd5a024522d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you suggest any ways to get promoted at work?\n",
            "\n",
            "A: We offer a variety of career paths for people who want to get ahead in the workforce, including:\n",
            "\n",
            "Cultivating an audience\n",
            "\n",
            "Working on social media\n",
            "\n",
            "Working in marketing\n",
            "\n",
            "Working on a media business\n",
            "\n",
            "If you're interested in becoming a career coach, check out our career coaching resources.\n",
            "\n",
            "What skills do you want to learn?\n",
            "\n",
            "A: We offer a variety of career paths\n"
          ]
        }
      ]
    }
  ]
}