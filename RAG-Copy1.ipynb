{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6abe6d-03b0-4335-b388-fefad3e0172d",
   "metadata": {},
   "source": [
    "# Retrieval augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a57245-e4fe-4b22-9d70-b419523b11a2",
   "metadata": {},
   "source": [
    "## Loading Documents\n",
    "A first step in RAG is to load document. You need a loader that supports the document type you are interested in. We use in this example Langchain, because it includes a collection of 60+ libraries for multiple types of documents and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03dc9db-b35b-40d7-b94f-013c6cc41d50",
   "metadata": {},
   "source": [
    "A first example with the `PyPDFLoader` library. Pdf support is direct and a single command is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "926d4fa1-897d-476a-a16a-caea89e1337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this loading Documents part, you may need these packages installed\n",
    "\n",
    "#!pip install langchain\n",
    "#!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba47b07-c15a-407e-abaf-06e13b147f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings # optional, disabling warnings about versions and others\n",
    "warnings.filterwarnings('ignore') # optional, disabling warnings about versions and others\n",
    "\n",
    "#!pip install pypdf \n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/War-of-the-Worlds.pdf\")\n",
    "book = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0155ef27-face-4f42-8359-b36b8aaf3eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long is the document we loaded?\n",
    "len(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be420767-f472-40a2-a239-dc61747a697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darkness were Ottershaw and Chertsey and a ll their hundreds of people, sleeping in \n",
      "peace.  \n",
      "   He was full of speculation that night a bout the condition of Mars, and scoffed at the \n",
      "vulgar idea of its having in- habitants w ho were signalling us. His idea was that \n",
      "meteorites might be falling in a heavy shower upon the planet, or that a huge volcanic \n",
      "explosion was in progress. He pointed out to me how unlikely it was that organic \n",
      "evolution had taken the same direction in the two adjacent pl\n"
     ]
    }
   ],
   "source": [
    "#Looking at a small extract, one page, and a few hundred characters in that page\n",
    "page = book[5]\n",
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78963c00-21cd-45fc-9b11-d4c8b695e917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'docs/War-of-the-Worlds.pdf', 'page': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Which page is it, from which document?\n",
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a16ab2-a33a-4dc3-a58d-8434d549e3a5",
   "metadata": {},
   "source": [
    "A second example with a Youtube video. There is a little more work here. The yt_dlp library will need options to know what audio format to download (we won't care much about the video part). Here we use m4a, at 192 kbps. Then the ffmpeg and ffprobe programs will isolate and stream the audio part. We will then use the OpenAI whisper library to covnert the audio into text (speech-to-text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92996683-f7ae-40bf-8a84-c1fbdbf33e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=2vkJ7v0x-Fs\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading webpage\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading ios player API JSON\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading web creator player API JSON\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading m3u8 information\n",
      "[info] 2vkJ7v0x-Fs: Downloading 1 format(s): 251\n",
      "[download] Destination: docs/youtube/Big Data Architectures.webm\n",
      "[download] 100% of   22.03MiB in 00:00:02 at 10.86MiB/s    \n",
      "[ExtractAudio] Destination: docs/youtube/Big Data Architectures.m4a\n",
      "Deleting original file docs/youtube/Big Data Architectures.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "#! pip install yt_dlp\n",
    "#! pip install pydub\n",
    "#!pip install ffmpeg\n",
    "#!pip install ffprobe\n",
    "#!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
    "\n",
    "import os\n",
    "import whisper\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# Step 1: Set up the download options\n",
    "url = \"https://www.youtube.com/watch?v=2vkJ7v0x-Fs\"\n",
    "save_dir = \"docs/youtube/\"\n",
    "output_template = os.path.join(save_dir, '%(title)s.%(ext)s')\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': output_template,  # Save the file to the specified directory with a title-based name\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',  # You can change this to mp3 if you prefer\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'ffmpeg_location': '/opt/homebrew/bin/ffmpeg',  # Specify the location of ffmpeg\n",
    "}\n",
    "\n",
    "# Step 2: Download the audio from the YouTube video\n",
    "with YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])\n",
    "\n",
    "# Step 3: Find the downloaded file\n",
    "downloaded_file = [f for f in os.listdir(save_dir) if f.endswith('.m4a')][0]  # Assuming m4a, adjust if using mp3\n",
    "downloaded_file_path = os.path.join(save_dir, downloaded_file)\n",
    "\n",
    "# Step 4: Load the Whisper model\n",
    "model = whisper.load_model(\"base\")  # You can choose 'tiny', 'base', 'small', 'medium', or 'large'\n",
    "\n",
    "# Step 5: Transcribe the audio file\n",
    "result = model.transcribe(downloaded_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2e169a-66bc-4f9e-bacb-05537633d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript saved to docs/youtube/transcript.txt\n"
     ]
    }
   ],
   "source": [
    "# Adding metadata to the transcript, and saving the transcript to a file so we can use it outside of this program.\n",
    "class Document:\n",
    "    def __init__(self, source, text, metadata=None):\n",
    "        self.source = source\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# Wrap the transcription result in the Document class with metadata\n",
    "document = Document(\n",
    "    source=downloaded_file_path,\n",
    "    text=result['text'], \n",
    "    metadata={\"source\": \"youtube\", \"file_path\": downloaded_file_path}\n",
    ")\n",
    "#Save the transcript to a text file\n",
    "transcript_file_path = os.path.join(save_dir, 'transcript.txt')\n",
    "with open(transcript_file_path, 'w') as f:\n",
    "    f.write(result['text'])\n",
    "\n",
    "print(f\"Transcript saved to {transcript_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e4c04b-8eb6-411a-9876-181447c55824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32857"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many characters in this transcript file?\n",
    "len(document.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6311ecd-14e7-43d0-ae65-9302e22a9b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In lesson four, we will go deeper into architectures for big data, and we will take a closer look at some of the most popular big data management systems. First, we're going to look at how the big data management system framework looks, and explore the commonalities that pretty much all the big data systems have, as well as some of the key differences between no SQL, MPP, and Hadoop. Next, we're going to take a deep dive into the Hadoop data management system. You will see how we both store dat\n"
     ]
    }
   ],
   "source": [
    "# Print the first 500 characters of the transcript\n",
    "print(document.page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d8b7b-a005-4431-9238-4647e4c89510",
   "metadata": {},
   "source": [
    "## Splitting our documents in chunks\n",
    "A second step is to split our documents (a 128-page book and 32K-character trasncript file) into smaller chunks. We use Langchain libraries here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afaa217-7968-4e82-b791-c4ca3c099b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first use 2 libraries, the first one is the most important, but we'll look at both\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5267876d-56c3-4515-950b-3622e770ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks have a character length, and an overlap values. For example (in real life, you are probably closer to 500 to 1000 and 50 to 100 respectively):\n",
    "chunk_size =20\n",
    "chunk_overlap = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59236a33-4e8a-41bc-90c7-8904cb12cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the character splitter and the recursive character splitter to give you an intuition on how splitting works. Let's create a function for each:\n",
    "rsplit = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "csplit = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae1f1dab-57b9-476b-8bcf-dd6cf610e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take an example string\n",
    "text1 = 'abcdefghijklmnopqrstuvwxyz1234567890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356c24ea-0d81-4cce-aff6-b1601e5515c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrst', 'pqrstuvwxyz123456789', '567890']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsplit.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc10a7ed-af57-472a-b937-94cf1ed6b3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz1234567890']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csplit.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4be2924-1ad9-4d62-bed3-76f550896a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character splitter does not do anything, because it considers by default the end of paragraph as the separator. You can specify additional separators. THis is true for any splitter, including REcursive Character. Let's add more separators to rsplit:\n",
    "# The separateors are by descending order of preference (try the first, if you can't get a chunk within the right size, try the second, etc.)\n",
    "rsplit = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2b9c8b9-f6ec-4a48-b1df-37238de79cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'a b c d e f g h i j k l m n o p q r s t u v w x y z 1 2 3 4 5 6 7 8 9 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbbaa20b-6015-47a5-a0b5-4b14a9199f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrst', 'pqrstuvwxyz123456789', '567890']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsplit.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eff90ba-9521-40d9-acd2-7ae9edb6d67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j',\n",
       " 'i j k l m n o p q r',\n",
       " 'q r s t u v w x y z',\n",
       " 'y z 1 2 3 4 5 6 7 8',\n",
       " '7 8 9 0']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsplit.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1ea8853-b4d0-4395-ab64-e6dd854c4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hamlet = \"\"\"Truly to speak, and with no addition, \\\n",
    "We go to gain a little patch of ground \\\n",
    "That hath in it no profit but the name. \\\n",
    "To pay five ducats, five, I would not farm it; \\\n",
    "Nor will it yield to Norway or the Pole \\\n",
    "A ranker rate, should it be sold in fee.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f250a326-40cf-4b99-8818-b388a489900e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Truly to speak, and',\n",
       " 'and with no',\n",
       " 'no addition, We go',\n",
       " 'go to gain a little',\n",
       " 'patch of ground',\n",
       " 'That hath in it no',\n",
       " 'no profit but the',\n",
       " 'the name. To pay',\n",
       " 'pay five ducats,',\n",
       " 'five, I would not',\n",
       " 'not farm it; Nor',\n",
       " 'Nor will it yield',\n",
       " 'to Norway or the',\n",
       " 'the Pole A ranker',\n",
       " 'rate, should it be',\n",
       " 'be sold in fee.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsplit.split_text(Hamlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "192d29ed-a7c9-4958-b8f3-b1eadc2cb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go for a more realistic chunk size\n",
    "rsplit = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6ba9199-d8cb-4514-88df-fd060a97225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the files, first the pdf\n",
    "rdoc1 = rsplit.split_documents(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbdd4440-fd97-47df-8a5c-ada64dbe0a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdoc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58d132c8-a45a-4ce3-96e5-bed41007aa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the splitted version has more documents (pages) than the original pdf source, \n",
    "len(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9be1616b-09a4-4d3c-9f75-7793daebf847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split 1 ---\n",
      "small and still, faintly marked with transver se stripes, and slightly flattened from the \n",
      "perfect round. But so little it was, so silvery warm--a pin's-head of li ght! It was as if it \n",
      "quivered, but really this was the telescope vi brating with the activity of the clockwork \n",
      "that kept the planet in view.     As I watched, the planet seemed to grow larger and smaller and to advance and recede, \n",
      "but that was simply that my eye was tired. Forty millions of miles it was from us--more\n",
      "\n",
      "--- Split 2 ---\n",
      "but that was simply that my eye was tired. Forty millions of miles it was from us--more \n",
      "than forty millions of miles of void. Few people realise the im- mensity of vacancy in \n",
      "which the dust of the material universe swims.  \n",
      "   Near it in the field, I re member, were three faint points of  light, three telescopic stars \n",
      "infinitely remote, and all around it was th e unfathomable darkness of empty space. You\n",
      "\n",
      "--- Split 3 ---\n",
      "infinitely remote, and all around it was th e unfathomable darkness of empty space. You \n",
      "know how that blackness looks on a frosty st arlight night. In a tele- scope it seems far \n",
      "profounder. And invisible to me because it wa s so remote and small, flying swiftly and \n",
      "steadily towards me across that incredible di stance, drawing nearer every min- ute by so \n",
      "many thousands of miles, came the Thing they  were sending us, the Thing that was to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing a few splits\n",
    "for i, doc in enumerate(rdoc1[30:33]):  # Adjust the number 3 to print more or fewer splits\n",
    "    print(f\"--- Split {i + 1} ---\")\n",
    "    print(doc.page_content)\n",
    "    print()  # Print an empty line for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f840a45f-5a73-47f5-a8ed-5d049e7c9491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split 1 ---\n",
      "how we actually execute analytics jobs on that data that's sitting in HDFS. So on the master node we have a new function, a new demon called the job tracker, and on the slave nodes we have a new one called the task tracker. Now let's say we have an application job that needs to communicate and analyze some data set that's sitting on the slave nodes down below. So the application job executes a Java command on the API, communicating with the name node, and then it tries to communicate down to\n",
      "\n",
      "--- Split 2 ---\n",
      "Java command on the API, communicating with the name node, and then it tries to communicate down to the task trackers below. Now one of the big differences between big data architectures and traditional data processing is that we don't try to bring all the data to one place and analyze it. What we do is we send the processing job down to the data and distribute it. You can think of it like having a lot of minions doing the work for you. One analogy might be if you had a very, very big\n",
      "\n",
      "--- Split 3 ---\n",
      "having a lot of minions doing the work for you. One analogy might be if you had a very, very big newspaper. Let's say you had a newspaper that was 10,000 pages long. And you wanted to find just one keyword in that newspaper. Well how would you do it? Well you could start on page one of that newspaper and read all the way through to page 10,000. And as you go you start to add up and count all the words that you're looking for. But that would take a very, very long time. But now what if you had a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the trasncript of the audio file\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Step 1: Load the transcript text\n",
    "transcript_file_path = \"docs/youtube/transcript.txt\"\n",
    "with open(transcript_file_path, 'r') as f:\n",
    "    transcript_text = f.read()\n",
    "\n",
    "# Step 2: Create a Document object\n",
    "document = Document(page_content=transcript_text)\n",
    "\n",
    "# Step 3: Split the transcript into chunks\n",
    "rdoc2 = rsplit.split_documents([document])\n",
    "\n",
    "# Step 4 manually assigning the metadata to each split\n",
    "save_dir = \"docs/youtube/\"\n",
    "downloaded_file = [f for f in os.listdir(save_dir) if f.endswith('.m4a')][0]  # Assuming m4a, adjust if using mp3\n",
    "downloaded_file_path = os.path.join(save_dir, downloaded_file)\n",
    "for doc in rdoc2:\n",
    "    doc.metadata = {\"source\": \"youtube\", \"file_path\": downloaded_file_path}\n",
    "\n",
    "\n",
    "# Step 5: Print the first few splits\n",
    "for i, doc in enumerate(rdoc2[30:33]):  # Adjust the number 3 to print more or fewer splits\n",
    "    print(f\"--- Split {i + 1} ---\")\n",
    "    print(doc.page_content)\n",
    "    print()  # Print an empty line for better readability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaa78678-8df5-4967-8d44-30efb621df92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for rdoc1:\n",
      "--- Metadata for Split 1 ---\n",
      "{'source': 'docs/War-of-the-Worlds.pdf', 'page': 1}\n",
      "\n",
      "--- Metadata for Split 2 ---\n",
      "{'source': 'docs/War-of-the-Worlds.pdf', 'page': 1}\n",
      "\n",
      "--- Metadata for Split 3 ---\n",
      "{'source': 'docs/War-of-the-Worlds.pdf', 'page': 1}\n",
      "\n",
      "Metadata for rdoc2:\n",
      "--- Metadata for Split 1 ---\n",
      "{'source': 'youtube', 'file_path': 'docs/youtube/Big Data Architectures.m4a'}\n",
      "\n",
      "--- Metadata for Split 2 ---\n",
      "{'source': 'youtube', 'file_path': 'docs/youtube/Big Data Architectures.m4a'}\n",
      "\n",
      "--- Metadata for Split 3 ---\n",
      "{'source': 'youtube', 'file_path': 'docs/youtube/Big Data Architectures.m4a'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the metadata\n",
    "\n",
    "# Viewing metadata of the first few splits from rdoc1 (the pdf text)\n",
    "print(\"Metadata for rdoc1:\")\n",
    "for i, doc in enumerate(rdoc1[:3]):  # Adjust the number to view more or fewer splits\n",
    "    print(f\"--- Metadata for Split {i + 1} ---\")\n",
    "    print(doc.metadata)  # Print the metadata\n",
    "    print()  # Print an empty line for better readability\n",
    "\n",
    "# Viewing metadata of the first few splits from rdoc2 (the video transcript)\n",
    "print(\"Metadata for rdoc2:\")\n",
    "for i, doc in enumerate(rdoc2[:3]):  # Adjust the number to view more or fewer splits\n",
    "    print(f\"--- Metadata for Split {i + 1} ---\")\n",
    "    print(doc.metadata)  # Print the metadata\n",
    "    print()  # Print an empty line for better readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32148e2-3c79-4eb8-943f-bb63fb4e804a",
   "metadata": {},
   "source": [
    "Recursive character splitting is a very common technique. But if you use an LLM that severly limits the number of input token (or charges you b y the token), you may want to split based on tokens instead of character sequences. This is how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9375610b-1526-4453-9fca-46982cca06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd44ea57-3fe6-4c81-8c9b-cf52c600d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a very small chunk and no overlap, so you can see what a chunk looks like with this method\n",
    "token_split = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04075986-8a0f-408b-a8b1-bef12f58be36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'ruly', ' to', ' speak', ',', ' and', ' with', ' no', ' addition', ',', ' We', ' go', ' to', ' gain', ' a', ' little', ' patch', ' of', ' ground', ' That', ' hath', ' in', ' it', ' no', ' profit', ' but', ' the', ' name', '.', ' To', ' pay', ' five', ' d', 'uc', 'ats', ',', ' five', ',', ' I', ' would', ' not', ' farm', ' it', ';', ' Nor', ' will', ' it', ' yield', ' to', ' Norway', ' or', ' the', ' Pole', ' A', ' rank', 'er', ' rate', ',', ' should', ' it', ' be', ' sold', ' in', ' fee', '.']\n"
     ]
    }
   ],
   "source": [
    "print(token_split.split_text(Hamlet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485dca5c-472e-4cce-bda5-7a700c424572",
   "metadata": {},
   "source": [
    "## Storing in Vector Store\n",
    "The third step is to store your splits in a vector database. There are dozens of solutions. Very popular solutions for local storage include Mongodb, Chroma, Weaviate and Milvus. All large Cloud vendors (Azure, AWS etc.) offer a Cloud vectordb solution. Here we use Chroma, a locally stored, flexible popular choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ae5f6-5d15-48d6-b9dc-6353fd4e706d",
   "metadata": {},
   "source": [
    "Before storing our data into the vectordb, we need to convert the text strings into vectors (embedding). We use a tokenizer compatible with the BERT model to first tokenize the text, then embed (convert to vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b832c59c-75a3-4ab6-a0a4-36a7258d91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama embeddings and vector store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "all_splits = rdoc1 + rdoc2\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc023e2-ab3e-4d50-a49a-60bb1ca5032c",
   "metadata": {},
   "source": [
    "What do these vectors look like? Let's play with a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15399743-b027-4b8b-99e9-508ad327ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"i like hotdogs\"\n",
    "text2 = \"i like sandwiches\"\n",
    "text3 = \"this is a large building\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "360ae3f0-d86d-44ac-9e87-81bc0f597873",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embeddings.embed_query(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8dd83f4-8a8c-4054-9d0d-90fbcee01fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embeddings.embed_query(text1)\n",
    "embedding2 = embeddings.embed_query(text2)\n",
    "embedding3 = embeddings.embed_query(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecbc5b00-af1c-45af-9bb7-2528052b012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding1 includes 768 values\n",
      "First few values: [-0.19900505244731903, -0.022423911839723587, -3.72220778465271, -0.7225621342658997, 0.05477889999747276, 0.9443159699440002, -1.1486680507659912, 0.5535013675689697, -0.9903378486633301, -0.840915322303772]\n"
     ]
    }
   ],
   "source": [
    "# looking at the first values of the first embedding\n",
    "print(\"embedding1 includes\", len(embedding1), \"values\")\n",
    "print(\"First few values:\", embedding1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21485d03-8e16-404e-982d-8dd292b1416c",
   "metadata": {},
   "source": [
    "How closes are these vectors from one another? There are many ways to compare them, here we use the dot product and the cosine similarity methods, which are common techniques for such comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2582a81b-4d1d-4136-b936-23a401a169ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (with dot product)between sentence 1 and 2: 0.7179325440433726\n",
      "Similarity (with dot product) between sentence 1 and 3: 0.3768511618446678\n"
     ]
    }
   ],
   "source": [
    "#Dot product method, comparing text1 to text2 vectors\n",
    "# Step 1 : creating the normalized vectors (so the product is between 0 and 1)\n",
    "import numpy as np\n",
    "norm_a = np.linalg.norm(embedding1)\n",
    "norm_b = np.linalg.norm(embedding2)\n",
    "norm_c = np.linalg.norm(embedding3)\n",
    "normalized_a = embedding1 / norm_a\n",
    "normalized_b = embedding2 / norm_b\n",
    "normalized_c = embedding3 / norm_c\n",
    "\n",
    "#Step 2: comparing text1 and text 2 embeddings, then text1 and text 3 embeddings:\n",
    "sim_1_2 = np.dot(normalized_a, normalized_b)\n",
    "sim_1_3 = np.dot(normalized_a, normalized_c)\n",
    "\n",
    "print(\"Similarity (with dot product)between sentence 1 and 2:\", sim_1_2)\n",
    "print(\"Similarity (with dot product) between sentence 1 and 3:\", sim_1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e2afb0f-944f-4b99-bfdb-fb6940a33a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (with cos similarity) between sentence 1 and 2: 0.7179325440433726\n",
      "Similarity (with cos similarity) between sentence 1 and 3: 0.37685116184466794\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "similarity_1_2 = cosine_similarity(embedding1, embedding2)\n",
    "similarity_1_3 = cosine_similarity(embedding1, embedding3)\n",
    "\n",
    "print(\"Similarity (with cos similarity) between sentence 1 and 2:\", similarity_1_2)\n",
    "print(\"Similarity (with cos similarity) between sentence 1 and 3:\", similarity_1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e43aaa-86a7-4e60-adeb-d276fd3da258",
   "metadata": {},
   "source": [
    "Now that we have embeddings, let's store them into a Chroma database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39875f5a-4dfb-47fd-88f2-5197d15392b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Set the environment variable to disable tokenizers parallelism and avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Let's define a directory where we'll store the database beyond this notebook execution (and let's make sure it is emtpy, as I run this notebook often :))\n",
    "persist_directory = 'docs/chroma/'\n",
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf6a4179-8f4c-4971-b201-85b08f3edd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef1288-a535-4058-89b8-d922781634d4",
   "metadata": {},
   "source": [
    "Now let's see if we can perform some similarity search with this database. keep in mind that we are just comparing vectors here, there is no LLM yet to smartly correlate deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5071c834-10b8-4acc-baca-fc1ae4fe022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Did the spaceship come from the planet Mars?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a8c9919-302b-472c-ba83-0a69d06a482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2299dabf-d085-4f22-be36-38eb8caa3996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65b18a45-8cdd-4037-a229-07312f0ed401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for the inhabitants of Mars. The immediate pr essure of necessity has brightened their \\nintellects, enlarged their pow ers, and hardened their h earts. And looking across space \\nwith instruments, and intelligences such as we have scarcely dreamed of, they see, at its \\nnearest distance only 35,000,000 of miles sunward of them, a morning star of hope, our \\nown warmer planet, green with vegetation and grey with water, w ith a cloudy atmosphere'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d124340-1dd2-4e56-908f-202fc48979f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/3954764980.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# Let's save the vectordb so we can use it outside of this notebook - note, this is FYI as it is automatically done with Chroma, but not with all other vectordbs!\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a3c38-f21e-4bb3-869b-5bbdd1b63b2e",
   "metadata": {},
   "source": [
    "## More on Similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e5924-65c2-4fc7-831d-791915c51e2c",
   "metadata": {},
   "source": [
    "The goal of the retrieval phase is to select the most relevant documents. But 'relevant' may mean 'repeating the same most relevant segment', which is suboptimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd9057c2-6ba3-4d11-988b-1595ff890972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting leftovers from previous instances, as I run this codebook often\n",
    "#tempdb.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f9ab1ed-bb31-482e-be00-fcbf858ac7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = [\n",
    "    \"\"\"The alien spaceships looked like flying saucers.\"\"\",\n",
    "    \"\"\"The alien spaceships were round in shape.\"\"\",\n",
    "    \"\"\"The spaceships were destroying everything.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8d35e6b-74e1-4913-a241-952682a6562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdb = Chroma.from_texts(text3, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b861ac74-c39b-4024-be3a-eeded47ecc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What can you tell me about the alien spaceships?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18228075-a21a-48ef-bacc-513720bb6a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The alien spaceships were round in shape.'),\n",
       " Document(metadata={}, page_content='The alien spaceships looked like flying saucers.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff4824-e55a-4232-a566-f565ef646fe3",
   "metadata": {},
   "source": [
    "Similarity search points to the documents that are closest semantically to the question, which may include a lot of redundant information, and miss some key points, for example that the alien spaceships were destroying everything. Max Marginal Relevance (MMR) search improves Similarity Search by picking the top k as Similarity Search does, but returning the vectors that are farthest from each other (in this top k list), so as to maximize the diversity of information returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5da5568b-0150-4e82-b45e-5e07537d9cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The alien spaceships were round in shape.'),\n",
       " Document(metadata={}, page_content='The spaceships were destroying everything.')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81833bb-f110-4f4d-ade4-c14a6d10ebb7",
   "metadata": {},
   "source": [
    "You may also want to filter the source where the information should be retrived from. You can achive such filtering manually, or automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a7e131b-b966-4918-a5d0-ce09b8d42caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual filtering method\n",
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\":\"docs/War-of-the-Worlds.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17f64605-6084-46ef-a54b-1f29ec811a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'point of view of an observer on Venus. Subsequently a peculiar luminous and sinuous \\nmark- ing appeared on the unillumined half of the inner planet, and almost simultaneously a faint dark mark of a si milar sinuous character was detected upon a \\nphotograph of the Martian disk. One needs to see the drawings of these ap- pearances in \\norder to appreciate fully their remark able resemblance in character.  \\nAt any rate, whether we expect another inva sion or not, our views of the human future'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef8da9-7d92-4281-a684-85b6dda2fa30",
   "metadata": {},
   "source": [
    "## Retrieving with the LLM in action\n",
    "The full process consists of asking a question, retrieving the relevant information, then passing the information and the question to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0dbccfd8-b360-492a-9430-c3b76f048c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/2605544752.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
     ]
    }
   ],
   "source": [
    "#We still need these bricks, so do not run this part of the notebook in isolation\n",
    "persist_directory = 'docs/chroma/'\n",
    "embedding = embeddings\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3867d02c-7875-41cd-a119-20e4f696b977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b62fe520-20ec-4f1b-b2e9-cf6bd4c85032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Did the spaceship come from the planet Mars?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "252cad83-2c21-4f36-96fa-f235bf2b3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama\n",
    "#!ollama serve & ollama pull llama3 & ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0e2be6d-4d44-4d5f-bc92-d1e54c47ffb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There is currently no conclusive evidence of the existence of aliens on Mars. However, there are ongoing efforts to search for signs of life, past or present, on the planet.\\n\\nNASA's Curiosity rover has been exploring Mars since 2012 and has found evidence of ancient lakes, rivers, and even an ocean on Mars in the distant past. This suggests that conditions may have been suitable for life to exist on Mars billions of years ago.\\n\\nIn 2020, NASA's Perseverance rover discovered methane in the Martian atmosphere, which could be a sign of microbial life. However, it's also possible that the methane is geological in origin, meaning it's not related to living organisms.\\n\\nThe European Space Agency's Schiaparelli lander and NASA's InSight lander have also been searching for signs of life on Mars. The InSight mission has provided valuable insights into the Martian interior and surface, but so far, there is no conclusive evidence of alien life.\\n\\nFuture missions, such as the NASA Mars 2020 rover and the European Space Agency's ExoMars rover, will continue to search for signs of life on Mars. These missions are equipped with instruments designed to detect biosignatures, such as atmospheric gases or geological features that could indicate the presence of life.\\n\\nWhile there is currently no evidence of aliens on Mars, the possibility of finding microbial life or even intelligent life in the future is intriguing and motivates continued exploration and research.\\n\\nSome interesting facts about Mars and the search for alien life:\\n\\n1. Mars has polar ice caps made up of water ice and dry ice (frozen carbon dioxide).\\n2. The planet's surface temperature can range from -125°C to 20°C (-200°F to 70°F).\\n3. NASA's Curiosity rover has been exploring Mars for over a decade, covering over 20 kilometers (12 miles) on the Martian surface.\\n4. The Mars 2020 rover is equipped with instruments designed to search for signs of microbial life, such as the Planetary Instrument for X-ray Lithochemistry (PIXL).\\n5. The ExoMars rover will be launched in the mid-2020s and will carry a drill capable of collecting samples from beneath the Martian surface.\\n\\nStay tuned for further updates on our understanding of Mars and the search for alien life!\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Llama3 as the LLM, and Ollama as the wrapper to interact with Llama3\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model = \"llama3\")\n",
    "llm.invoke(\"Are there aliens on Mars?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "529ca15f-1519-491d-8b1b-0c5f5be3e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama langchain beautifulsoup4 chromadb gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5e4f3da-8007-4b84-b7fd-15647e5981d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is \"almost\" the final code.\n",
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Create Ollama embeddings and vector store\n",
    "#embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "#vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Define the function to call the Ollama Llama3 model\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Define the RAG setup\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "# Define the Gradio interface\n",
    "def get_important_facts(question):\n",
    "    return rag_chain(question)\n",
    "\n",
    "# Create a Gradio app interface\n",
    "iface = gr.Interface(\n",
    "  fn=get_important_facts,\n",
    "  inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "  outputs=\"text\",\n",
    "  title=\"RAG with Llama3\",\n",
    "  description=\"Ask questions about the proveded context\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n",
    "# example q: did the aliens eventually go on to land on Venus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b25a052-36de-485f-b072-964abcfbccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ollama_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Instantiate the custom LLM class\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m ollama_llm_instance \u001b[38;5;241m=\u001b[39m OllamaLLM(\u001b[43mollama_llm\u001b[49m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Define the conversational retrieval chain\u001b[39;00m\n\u001b[1;32m     64\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[1;32m     65\u001b[0m     llm\u001b[38;5;241m=\u001b[39mollama_llm_instance,\n\u001b[1;32m     66\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m     67\u001b[0m     memory\u001b[38;5;241m=\u001b[39mmemory \u001b[38;5;66;03m# add the memory module, to pass the previous exchange to the LLM as well\u001b[39;00m\n\u001b[1;32m     68\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ollama_llm' is not defined"
     ]
    }
   ],
   "source": [
    "# Now, Ollama re-injects the previouys question and asnwer into the model with the next question. But other LLMs would forget the previous question (remember Dolly?) You can add memory with a memory module.\n",
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define memory to store the previous exchanges\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Custom Runnable LLM class with debugging\n",
    "class OllamaLLM(Runnable):\n",
    "    def __init__(self, llm_fn):\n",
    "        self.llm_fn = llm_fn\n",
    "\n",
    "    def invoke(self, input, config=None, **kwargs):\n",
    "        # If the input is a StringPromptValue or similar object, treat it as a string\n",
    "        question = str(input)  # Convert the input to a string if necessary\n",
    "        context = kwargs.get(\"context\", \"\")  # Retrieve context from kwargs if available\n",
    "\n",
    "        # Print what was passed to the LLM\n",
    "        print(f\"Question passed to LLM: {question}\")\n",
    "        print(f\"Context passed to LLM: {context}\")\n",
    "\n",
    "        # Handle additional kwargs such as stop, if needed\n",
    "        stop = kwargs.get(\"stop\", None)\n",
    "\n",
    "        # If 'stop' or other arguments need to be passed to the LLM function, handle them here\n",
    "        response = self.llm_fn(question, context)\n",
    "        \n",
    "        # Print the response from the LLM\n",
    "        print(f\"Response from LLM: {response}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    def predict(self, input, **kwargs):\n",
    "        return self.invoke(input, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.invoke(*args, **kwargs)\n",
    "\n",
    "# Instantiate the custom LLM class\n",
    "ollama_llm_instance = OllamaLLM(ollama_llm)\n",
    "\n",
    "# Define the conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ollama_llm_instance,\n",
    "    retriever=retriever,\n",
    "    memory=memory # add the memory module, to pass the previous exchange to the LLM as well\n",
    ")\n",
    "\n",
    "# Define the function to get important facts with debugging\n",
    "def get_important_facts(question):\n",
    "    # Print what is passed to the retriever\n",
    "    print(f\"Question passed to retriever: {question}\")\n",
    "    \n",
    "    # Run the chain and capture the memory state\n",
    "    response = qa_chain.run({\"question\": question})\n",
    "    \n",
    "    # Print what is in memory after the retrieval\n",
    "    print(f\"Memory state: {memory.buffer}\")\n",
    "\n",
    "    return response\n",
    "\n",
    "# Create a Gradio app interface\n",
    "iface = gr.Interface(\n",
    "    fn=get_important_facts,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"RAG with Llama3\",\n",
    "    description=\"Ask questions about the provided context\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267c82c-af27-49dc-b231-322c8d648f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba7a1d-18e5-42af-bd53-367152253527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974abe75-57b3-4149-abe4-3cabf87fdffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1224c-26e1-4e4d-b182-b22c7b5c8211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e093d35-fefd-4977-ab74-cb290531ea24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23588122-14e7-40b2-932d-b5ef32a3a6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb3899bf-5976-4068-bd09-c0b4e147d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9907b3eb-d5e6-41dd-97bc-71771ed03c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_current_temperature.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8778b732-e3d6-4116-9e89-107e004fecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_current_temperature.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cd30565-a340-47f6-a4bc-f1bd028f1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_current_temperature.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ab4fe96-b213-4826-8f10-4b9382ea0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_current_temperature({\"latitude\": 13, \"longitude\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4745b5fb-127d-4cd0-acd9-4b6309c8b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the input schema\n",
    "class CityInput(BaseModel):\n",
    "    city: str = Field(..., description=\"City name to fetch weather data for\")\n",
    "\n",
    "# Tool to get the current weather\n",
    "@tool(args_schema=CityInput)\n",
    "def get_current_weather(city: str) -> dict:\n",
    "    \"\"\"Fetch current weather for a given city.\"\"\"\n",
    "    \n",
    "    API_KEY = '56e413d9ee2c3d177f933b2b57ff835e'  # Replace with your OpenWeatherMap API key\n",
    "    BASE_URL = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    \n",
    "    # Parameters for the weather request\n",
    "    params = {\n",
    "        'q': city,\n",
    "        'appid': API_KEY,\n",
    "        'units': 'metric',  # To get the temperature in Celsius\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        temperature = results['main']['temp']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        humidity = results['main']['humidity']\n",
    "        wind_speed = results['wind']['speed']\n",
    "        pressure = results['main']['pressure']\n",
    "    else:\n",
    "        raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    return (\n",
    "        f\"The current temperature in {city} is {temperature}°C with {weather_description}.\\n\"\n",
    "        f\"Humidity: {humidity}%, Wind Speed: {wind_speed} m/s, Pressure: {pressure} hPa.\"\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "# weather = get_current_weather(\"San Francisco\")\n",
    "# print(weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87e18314-833a-43c9-8f59-4144e8e4ccf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/3015745508.py:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use invoke instead.\n",
      "  get_current_weather(\"Richmond\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current temperature in Richmond is 26.89°C with scattered clouds.\\nHumidity: 57%, Wind Speed: 5.14 m/s, Pressure: 1022 hPa.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather(\"Richmond\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7711ea7e-ed40-47c9-b61c-8b656609f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integrating the weather in the full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c39c6ce1-0975-4238-b353-d216f28c071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question passed to retriever: Did the aliens eventually land on Venus?\n",
      "Question passed to LLM: text=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nLessing has advanced excellent reasons fo r supposing that the Martians have actually \\nsucceeded in effecting a landing on the planet  Venus. Seven months ago now, Venus and \\nMars were in alignment with the sun; that  is to say, Mars was in opposition from the \\npoint of view of an observer on Venus. Subsequently a peculiar luminous and sinuous\\n\\npoint of view of an observer on Venus. Subsequently a peculiar luminous and sinuous \\nmark- ing appeared on the unillumined half of the inner planet, and almost simultaneously a faint dark mark of a si milar sinuous character was detected upon a \\nphotograph of the Martian disk. One needs to see the drawings of these ap- pearances in \\norder to appreciate fully their remark able resemblance in character.  \\nAt any rate, whether we expect another inva sion or not, our views of the human future\\n\\nexisted beyond the petty surface of our minut e sphere. Now we see further. If the \\nMartians can reach Venus, there is no reason to suppose that the thing is impossible for \\nmen, and when the slow cooling of the sun make s this earth uninhabitable, as at last it \\nmust do, it may be that the thread of life th at has begun here will have streamed out and \\ncaught our sister planet within its toils.  \\n   Dim and wonderful is the vision I have conj ured up in my mind of  life spreading slowly\\n\\nit has done much to promote th e conception of the commonweal of mankind. It may be \\nthat across the immensity of space the Martians have watched the fate of these pioneers of theirs and learned their lesson, and that on the planet Venus they have found a securer \\nsettlement. Be that as it may, for many year s yet there will certainly be no relaxation of \\nthe eager scrutiny of the Martian disk, and thos e fiery darts of the sky, the shooting stars,\\n\\nQuestion: Did the aliens eventually land on Venus?\\nHelpful Answer:\"\n",
      "Context passed to LLM: \n",
      "Response from LLM: The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\n",
      "Memory state: [HumanMessage(content='Did the aliens eventually land on Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\", additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: Where does the action take place?\n",
      "Question passed to LLM: text=\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Did the aliens eventually land on Venus?\\nAssistant: The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\\nFollow Up Input: Where does the action take place?\\nStandalone question:\"\n",
      "Context passed to LLM: \n",
      "Response from LLM: Based on the chat history, the follow-up input \"Where does the action take place?\" can be rephrased to a standalone question in its original language as:\n",
      "\n",
      "\"Does the action take place on Earth or other planets?\"\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\npublic-houses, and here and there a messenger,  or even an eye-witness of the later \\noccurrences, caused a whirl of excitement, a shouting, and a running to and fro; but for \\nthe most part the daily rou tine of working, eating, drinking , sleeping, went on as it had \\ndone for count- less years--as t hough no planet Mars existed in the sky.  Even at Woking \\nstation and Horsell and Chobham that was the case.\\n\\nAbout three o\\'clock there began the thud of a gu n at measured intervals from Chertsey or \\nAddlestone. I learned that the smouldering pine wood into which the second cylin- der \\nhad fallen was being shelled, in the hope of destroying that ob ject before it opened. It was \\nonly about five, however, that a field gun r eached Chobham for use against the first body \\nof Martians.     About six in the evening, as I sat at tea with my wife in the summerhouse talking\\n\\nEnglish readers heard of it firs t in the issue of NATURE date d August 2. I am inclined to \\nthink that this blaze may have  been the casting of the huge g un, in the vast pit sunk into \\ntheir planet, from which th eir shots were fired at us . Peculiar markings, as yet \\nunexplained, were seen near the site of that outbreak during the next two oppositions.  \\n   The storm burst upon us six years ago now . As Mars approached opposition, Lavelle of\\n\\nprobable developments of moral id eas as civilisation progressed.  \\n   One night (the first missile then c ould scarcely have been 10,000,000 miles away) I \\nwent for a walk with my wife. It was starlig ht and I explained the Signs of the Zodiac to \\nher, and pointed out Mars, a bright dot of light creeping zenithwa rd, towards which so \\nmany telescopes were pointed. It was a warm night. Coming home, a party of excursionists from Chertsey or Isleworth passed us singing and playing music. There\\n\\nQuestion: Based on the chat history, the follow-up input \"Where does the action take place?\" can be rephrased to a standalone question in its original language as:\\n\\n\"Does the action take place on Earth or other planets?\"\\nHelpful Answer:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.\n",
      "Memory state: [HumanMessage(content='Did the aliens eventually land on Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Where does the action take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: What is the main city in the book?\n",
      "Question passed to LLM: text=\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Did the aliens eventually land on Venus?\\nAssistant: The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\\nHuman: Where does the action take place?\\nAssistant: The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.\\nFollow Up Input: What is the main city in the book?\\nStandalone question:\"\n",
      "Context passed to LLM: \n",
      "Response from LLM: The context is that you want to rephrase a follow-up question from a conversation between a human and an assistant into a standalone question in its original language. The conversation involves discussing a passage about extraterrestrial life, specifically Martians landing on Venus or other planets. The human asks the assistant where the action takes place, and the assistant explains that the context is a passage describing events related to the approach of Mars and its impact on human life.\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\ngas float. It required a certain amount of scientific educati on to perceive that the grey \\nscale of the Thing was no common oxide, that the yellowish-white metal that gleamed in \\nthe crack between the lid and the cylinder ha d an unfamiliar hue. \"Extra-terrestrial\" had \\nno meaning for most of the onlookers.  \\n   At that time it was quite clear in my own mind that the Thing had come from the planet \\nMars, but I judged it improbable that it co ntained any living creature. I thought the\\n\\ntheir physiology and structur e is purely scientific.  \\n   A question of graver and universal interest is the possi- bility of another attack from the \\nMartians. I do not think that nearly enough atten tion is being given to this aspect of the \\nmatter. At present the planet Mars is in c onjunction, but with every return to opposition I, \\nfor one, anticipate a renewal of their adventur e. In any case, we should be prepared. It\\n\\nfor the inhabitants of Mars. The immediate pr essure of necessity has brightened their \\nintellects, enlarged their pow ers, and hardened their h earts. And looking across space \\nwith instruments, and intelligences such as we have scarcely dreamed of, they see, at its \\nnearest distance only 35,000,000 of miles sunward of them, a morning star of hope, our \\nown warmer planet, green with vegetation and grey with water, w ith a cloudy atmosphere\\n\\nexisted beyond the petty surface of our minut e sphere. Now we see further. If the \\nMartians can reach Venus, there is no reason to suppose that the thing is impossible for \\nmen, and when the slow cooling of the sun make s this earth uninhabitable, as at last it \\nmust do, it may be that the thread of life th at has begun here will have streamed out and \\ncaught our sister planet within its toils.  \\n   Dim and wonderful is the vision I have conj ured up in my mind of  life spreading slowly\\n\\nQuestion: The context is that you want to rephrase a follow-up question from a conversation between a human and an assistant into a standalone question in its original language. The conversation involves discussing a passage about extraterrestrial life, specifically Martians landing on Venus or other planets. The human asks the assistant where the action takes place, and the assistant explains that the context is a passage describing events related to the approach of Mars and its impact on human life.\\nHelpful Answer:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: The text describes the scene where the Thing (an extraterrestrial object) has landed, and the humans are trying to understand its composition and implications. The main character is reflecting on the possibility of future attacks from Martians and the potential for human life to spread to other planets, such as Venus.\n",
      "Memory state: [HumanMessage(content='Did the aliens eventually land on Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Where does the action take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the main city in the book?', additional_kwargs={}, response_metadata={}), AIMessage(content='The text describes the scene where the Thing (an extraterrestrial object) has landed, and the humans are trying to understand its composition and implications. The main character is reflecting on the possibility of future attacks from Martians and the potential for human life to spread to other planets, such as Venus.', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: In which city in the novel does the action take place?\n",
      "Question passed to LLM: text=\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Did the aliens eventually land on Venus?\\nAssistant: The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\\nHuman: Where does the action take place?\\nAssistant: The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.\\nHuman: What is the main city in the book?\\nAssistant: The text describes the scene where the Thing (an extraterrestrial object) has landed, and the humans are trying to understand its composition and implications. The main character is reflecting on the possibility of future attacks from Martians and the potential for human life to spread to other planets, such as Venus.\\nFollow Up Input: In which city in the novel does the action take place?\\nStandalone question:\"\n",
      "Context passed to LLM: \n",
      "Response from LLM: Based on the provided chat history and follow-up input, I rephrased the standalone question as follows:\n",
      "\n",
      "\"In what city does the action take place?\"\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\ndescribed the behaviour of the men and women to whom I spoke. A ll over the district \\npeople were dining and supping; working men were gardening after the labours of the \\nday, children were being put to bed, young people were wandering through the lanes \\nlove-making, stu- dents sat over their books.  \\n   Maybe there was a murmur in the village streets, a novel and dominant topic in the \\npublic-houses, and here and there a messenger,  or even an eye-witness of the later\\n\\ncould, I hurried on after the artillery- man. At the corner I looked back. The soldier had \\nleft him, and he was still standing by his box, wi th the pots of orchids on the lid of it, and \\nstaring vaguely over the trees.  \\n    No one in Weybridge could tell us wher e the headquarters were established; the whole \\nplace was in such confusion as I had never seen in any town before. Carts, carriages \\nevery- where, the most astonishing mis cellany of conveyances and horseflesh. The\\n\\n\"Where\\'s your shells?\" said the first speaker. \"There ain\\'t no time. Do it in a rush, that\\'s \\nmy tip, and do it at once.\"     So they discussed it. After a while I left them, and went on to th e railway station to get \\nas many morning papers as I could.  \\n    But I will not weary the reader with a description of that long morning and of the \\nlonger afternoon. I did not succeed  in getting a glimpse of th e common, for even Horsell\\n\\nAlready they were busy with returning people; in places even there were shops open, \\nand I saw a drinking foun tain running water.  \\n   I remember how mockingly bright the da y seemed as I went back on my melancholy \\npilgrimage to the little house at Woking, how  busy the streets and vivid the moving life \\nabout me. So many people were abroad ever ywhere, busied in a t housand activities, that \\nit seemed incredible that any great proporti on of the population could have been slain.\\n\\nQuestion: Based on the provided chat history and follow-up input, I rephrased the standalone question as follows:\\n\\n\"In what city does the action take place?\"\\nHelpful Answer:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: The context is a passage from H.G. Wells' science fiction novel \"The War of the Worlds\". The passage describes the chaos and destruction caused by an alien invasion in the early 20th century, specifically in the districts around Woking and Weybridge.\n",
      "\n",
      "The action takes place in Woking and its surrounding areas, including the village streets, lanes, public-houses, gardens, and railway station.\n",
      "Memory state: [HumanMessage(content='Did the aliens eventually land on Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Where does the action take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the main city in the book?', additional_kwargs={}, response_metadata={}), AIMessage(content='The text describes the scene where the Thing (an extraterrestrial object) has landed, and the humans are trying to understand its composition and implications. The main character is reflecting on the possibility of future attacks from Martians and the potential for human life to spread to other planets, such as Venus.', additional_kwargs={}, response_metadata={}), HumanMessage(content='In which city in the novel does the action take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage from H.G. Wells\\' science fiction novel \"The War of the Worlds\". The passage describes the chaos and destruction caused by an alien invasion in the early 20th century, specifically in the districts around Woking and Weybridge.\\n\\nThe action takes place in Woking and its surrounding areas, including the village streets, lanes, public-houses, gardens, and railway station.', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 108, in get_important_facts\n",
      "    return get_current_weather(city)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 35, in get_current_weather\n",
      "    raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
      "Exception: Weather API Request failed with status code: 404\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 108, in get_important_facts\n",
      "    return get_current_weather(city)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 35, in get_current_weather\n",
      "    raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
      "Exception: Weather API Request failed with status code: 404\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 108, in get_important_facts\n",
      "    return get_current_weather(city)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 35, in get_current_weather\n",
      "    raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
      "Exception: Weather API Request failed with status code: 404\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 108, in get_important_facts\n",
      "    return get_current_weather(city)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 35, in get_current_weather\n",
      "    raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
      "Exception: Weather API Request failed with status code: 404\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 108, in get_important_facts\n",
      "    return get_current_weather(city)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 35, in get_current_weather\n",
      "    raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
      "Exception: Weather API Request failed with status code: 404\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 108, in get_important_facts\n",
      "    return get_current_weather(city)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 35, in get_current_weather\n",
      "    raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
      "Exception: Weather API Request failed with status code: 404\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 108, in get_important_facts\n",
      "    return get_current_weather(city)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/1885066821.py\", line 35, in get_current_weather\n",
      "    raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
      "Exception: Weather API Request failed with status code: 404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question passed to retriever: Does part of the novel take place in England?\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Did the aliens eventually land on Venus?\\nAssistant: The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets\\' surfaces.\\nHuman: Where does the action take place?\\nAssistant: The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.\\nHuman: What is the main city in the book?\\nAssistant: The text describes the scene where the Thing (an extraterrestrial object) has landed, and the humans are trying to understand its composition and implications. The main character is reflecting on the possibility of future attacks from Martians and the potential for human life to spread to other planets, such as Venus.\\nHuman: In which city in the novel does the action take place?\\nAssistant: The context is a passage from H.G. Wells\\' science fiction novel \"The War of the Worlds\". The passage describes the chaos and destruction caused by an alien invasion in the early 20th century, specifically in the districts around Woking and Weybridge.\\n\\nThe action takes place in Woking and its surrounding areas, including the village streets, lanes, public-houses, gardens, and railway station.\\nFollow Up Input: Does part of the novel take place in England?\\nStandalone question:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: A fun challenge!\n",
      "\n",
      "Given the conversation history and the follow-up question, I would rephrase the standalone question as:\n",
      "\n",
      "\"Does part of the novel take place in England?\"\n",
      "\n",
      "This question is a direct translation of the original follow-up input, preserving the same wording and intent.\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nI go to London and see the busy multitudes in Fleet Street and the Strand, and it comes \\nacross my mind that they are but the ghosts of  the past, haunting the streets that I have \\nseen silent and wretched, going to and fro, phan- tasms in a d ead city, the mockery of life \\nin a galvanised body. And strange, too, it is to stand on Primrose Hill, as I did but a day before writing this last chapter, to see the great province of houses , dim and blue through\\n\\nmust have been used for some reason. A nd in the garden beyond Roehampton I got a \\nquan- tity of immature potat oes, sufficient to stay my hunger. From this garden one \\nlooked down upon Putney and the river. The aspect of the place in the dusk was \\nsingularly desolate: blackened trees, blackened, de solate ruins, and down the hill the \\nsheets of the flooded river, red-tinged with th e weed. And over all--sil ence. It filled me\\n\\nby means of occasional ruins of its villas and fe nces and lamps, and so presently I got out \\nof this spate and made my way to the hill  going up towards Roehampton and came out on \\nPutney Common.  \\n   Here the scenery changed from the st range and unfamiliar to the wreckage of the \\nfamiliar: patches of ground exhibited the deva station of a cyclone, and in a few score \\nyards I would come upon perfectly undisturbed spaces, houses with their blinds trimly\\n\\nthe top of West Hill a lot of blood-stained glass about the overtur ned water trough. My \\nmovements were languid, my plans of the vague st. I had an idea of going to Leatherhead, \\nthough I knew that there I ha d the poorest chance of finding my wife. Certainly, unless \\ndeath had overtaken them sud- denly, my cous ins and she would have fled thence; but it \\nseemed to me I might find or learn there whither the Surrey people had fled. I knew I\\n\\nQuestion: A fun challenge!\\n\\nGiven the conversation history and the follow-up question, I would rephrase the standalone question as:\\n\\n\"Does part of the novel take place in England?\"\\n\\nThis question is a direct translation of the original follow-up input, preserving the same wording and intent.\\nHelpful Answer:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: The passage is an excerpt from a novel, likely written by Virginia Woolf. The text describes the narrator's observations and reflections on their experiences in London and Surrey, including visits to Fleet Street, Primrose Hill, Roehampton, Putney Common, and Leatherhead.\n",
      "Memory state: [HumanMessage(content='Did the aliens eventually land on Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Where does the action take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the main city in the book?', additional_kwargs={}, response_metadata={}), AIMessage(content='The text describes the scene where the Thing (an extraterrestrial object) has landed, and the humans are trying to understand its composition and implications. The main character is reflecting on the possibility of future attacks from Martians and the potential for human life to spread to other planets, such as Venus.', additional_kwargs={}, response_metadata={}), HumanMessage(content='In which city in the novel does the action take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage from H.G. Wells\\' science fiction novel \"The War of the Worlds\". The passage describes the chaos and destruction caused by an alien invasion in the early 20th century, specifically in the districts around Woking and Weybridge.\\n\\nThe action takes place in Woking and its surrounding areas, including the village streets, lanes, public-houses, gardens, and railway station.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Does part of the novel take place in England?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The passage is an excerpt from a novel, likely written by Virginia Woolf. The text describes the narrator's observations and reflections on their experiences in London and Surrey, including visits to Fleet Street, Primrose Hill, Roehampton, Putney Common, and Leatherhead.\", additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: Does part of the novel take place in London?\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Did the aliens eventually land on Venus?\\nAssistant: The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets\\' surfaces.\\nHuman: Where does the action take place?\\nAssistant: The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.\\nHuman: What is the main city in the book?\\nAssistant: The text describes the scene where the Thing (an extraterrestrial object) has landed, and the humans are trying to understand its composition and implications. The main character is reflecting on the possibility of future attacks from Martians and the potential for human life to spread to other planets, such as Venus.\\nHuman: In which city in the novel does the action take place?\\nAssistant: The context is a passage from H.G. Wells\\' science fiction novel \"The War of the Worlds\". The passage describes the chaos and destruction caused by an alien invasion in the early 20th century, specifically in the districts around Woking and Weybridge.\\n\\nThe action takes place in Woking and its surrounding areas, including the village streets, lanes, public-houses, gardens, and railway station.\\nHuman: Does part of the novel take place in England?\\nAssistant: The passage is an excerpt from a novel, likely written by Virginia Woolf. The text describes the narrator\\'s observations and reflections on their experiences in London and Surrey, including visits to Fleet Street, Primrose Hill, Roehampton, Putney Common, and Leatherhead.\\nFollow Up Input: Does part of the novel take place in London?\\nStandalone question:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\n",
      "\n",
      "Follow-up question: Does part of the novel take place in London?\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nOf the falling of the fift h cylinder I have presently to tell. The sixth star fell at \\nWimbledon. My brother, keeping watch beside  the women in the chaise in a meadow, \\nsaw the green flash of it far beyond the hills.  On Tuesday the little  party, still set upon \\ngetting across the sea, made its way through the swarming country towards Colchester. \\nThe news that the Martians were now in possession of the whole of London was\\n\\nLessing has advanced excellent reasons fo r supposing that the Martians have actually \\nsucceeded in effecting a landing on the planet  Venus. Seven months ago now, Venus and \\nMars were in alignment with the sun; that  is to say, Mars was in opposition from the \\npoint of view of an observer on Venus. Subsequently a peculiar luminous and sinuous\\n\\nWindsor. Great anxiety prevails in West Su rrey, and earthworks are being thrown up to \\ncheck the advance Londonward.\" That was how the Sunday SUN put it, and a clever and remarkably prompt \"handbook\" article in the REFEREE compared the affair to a \\nmenagerie suddenly let loose in a village.  \\n   No one in London knew positively of the nature of the armoured Martians, and there \\nwas still a fixed idea that these monsters  must be sluggish: \"crawling,\" \"creeping\\n\\npublic-houses, and here and there a messenger,  or even an eye-witness of the later \\noccurrences, caused a whirl of excitement, a shouting, and a running to and fro; but for \\nthe most part the daily rou tine of working, eating, drinking , sleeping, went on as it had \\ndone for count- less years--as t hough no planet Mars existed in the sky.  Even at Woking \\nstation and Horsell and Chobham that was the case.\\n\\nQuestion: The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets\\' surfaces.\\n\\nFollow-up question: Does part of the novel take place in London?\\nHelpful Answer:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: The context is a passage from the novel \"The War of the Worlds\" by H.G. Wells. The passage describes the chaos and uncertainty that occurs as people learn about the Martian invasion of Earth, specifically the news that the Martians have taken possession of London.\n",
      "Memory state: [HumanMessage(content='Did the aliens eventually land on Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The context is a passage from a text about the possibility of extraterrestrial life landing on Earth or other planets. The passage discusses the idea that Martians may have landed on Venus, and mentions the alignment of Mars and Venus with the sun, as well as observations of markings on both planets' surfaces.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Where does the action take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage of text that describes events and observations related to the approach of Mars (Planet Mars) and its impact on human life. The narrator provides details about daily routines, military actions, and personal experiences during this time period.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the main city in the book?', additional_kwargs={}, response_metadata={}), AIMessage(content='The text describes the scene where the Thing (an extraterrestrial object) has landed, and the humans are trying to understand its composition and implications. The main character is reflecting on the possibility of future attacks from Martians and the potential for human life to spread to other planets, such as Venus.', additional_kwargs={}, response_metadata={}), HumanMessage(content='In which city in the novel does the action take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage from H.G. Wells\\' science fiction novel \"The War of the Worlds\". The passage describes the chaos and destruction caused by an alien invasion in the early 20th century, specifically in the districts around Woking and Weybridge.\\n\\nThe action takes place in Woking and its surrounding areas, including the village streets, lanes, public-houses, gardens, and railway station.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Does part of the novel take place in England?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The passage is an excerpt from a novel, likely written by Virginia Woolf. The text describes the narrator's observations and reflections on their experiences in London and Surrey, including visits to Fleet Street, Primrose Hill, Roehampton, Putney Common, and Leatherhead.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Does part of the novel take place in London?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage from the novel \"The War of the Worlds\" by H.G. Wells. The passage describes the chaos and uncertainty that occurs as people learn about the Martian invasion of Earth, specifically the news that the Martians have taken possession of London.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.runnables import Runnable\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Weather tool function\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"Fetch current weather for a given city.\"\"\"\n",
    "    \n",
    "    API_KEY = '56e413d9ee2c3d177f933b2b57ff835e'  # Replace with your OpenWeatherMap API key\n",
    "    BASE_URL = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    \n",
    "    params = {\n",
    "        'q': city,\n",
    "        'appid': API_KEY,\n",
    "        'units': 'metric',  # To get the temperature in Celsius\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        temperature = results['main']['temp']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        humidity = results['main']['humidity']\n",
    "        wind_speed = results['wind']['speed']\n",
    "        pressure = results['main']['pressure']\n",
    "    else:\n",
    "        raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    return (\n",
    "        f\"The current temperature in {city} is {temperature}°C with {weather_description}.\\n\"\n",
    "        f\"Humidity: {humidity}%, Wind Speed: {wind_speed} m/s, Pressure: {pressure} hPa.\"\n",
    "    )\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define memory to store the previous exchanges\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Custom Runnable LLM class with debugging\n",
    "class OllamaLLM(Runnable):\n",
    "    def __init__(self, llm_fn):\n",
    "        self.llm_fn = llm_fn\n",
    "\n",
    "    def invoke(self, input, config=None, **kwargs):\n",
    "        question = str(input)\n",
    "        context = kwargs.get(\"context\", \"\")\n",
    "        print(f\"Question passed to LLM: {question}\")\n",
    "        print(f\"Context passed to LLM: {context}\")\n",
    "        stop = kwargs.get(\"stop\", None)\n",
    "        response = self.llm_fn(question, context)\n",
    "        print(f\"Response from LLM: {response}\")\n",
    "        return response\n",
    "\n",
    "    def predict(self, input, **kwargs):\n",
    "        return self.invoke(input, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.invoke(*args, **kwargs)\n",
    "\n",
    "# Instantiate the custom LLM class\n",
    "ollama_llm_instance = OllamaLLM(ollama_llm)\n",
    "\n",
    "# Define the conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ollama_llm_instance,\n",
    "    retriever=retriever,\n",
    "    memory=memory  # add the memory module to pass the previous exchange to the LLM as well\n",
    ")\n",
    "\n",
    "# Function to detect if the question is about the weather in a city\n",
    "def is_weather_question(question: str) -> bool:\n",
    "    return \"weather\" in question.lower() and \"in\" in question.lower()\n",
    "\n",
    "# Extract city name from the weather question\n",
    "def extract_city_from_question(question: str) -> str:\n",
    "    # Simple heuristic to extract city name\n",
    "    if \"weather in\" in question.lower():\n",
    "        return question.lower().split(\"weather in\")[1].strip().split()[0].capitalize()\n",
    "    return \"\"\n",
    "\n",
    "# Define the function to get important facts with debugging\n",
    "def get_important_facts(question):\n",
    "    # Check if the question is about the weather in a city\n",
    "    if is_weather_question(question):\n",
    "        city = extract_city_from_question(question)\n",
    "        if city:\n",
    "            return get_current_weather(city)\n",
    "        else:\n",
    "            return \"I couldn't determine the city you're asking about. Please specify the city.\"\n",
    "    \n",
    "    # Otherwise, use the LLM-based chain\n",
    "    print(f\"Question passed to retriever: {question}\")\n",
    "    response = qa_chain.run({\"question\": question})\n",
    "    print(f\"Memory state: {memory.buffer}\")\n",
    "    return response\n",
    "\n",
    "# Create a Gradio app interface\n",
    "iface = gr.Interface(\n",
    "    fn=get_important_facts,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"RAG with Llama3\",\n",
    "    description=\"Ask questions about the provided context\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "885daa78-9720-4690-87b2-5f9c76615b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question passed to retriever: did the aliens eventually make it to Venus?\n",
      "Question passed to LLM: text=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\npoint of view of an observer on Venus. Subsequently a peculiar luminous and sinuous \\nmark- ing appeared on the unillumined half of the inner planet, and almost simultaneously a faint dark mark of a si milar sinuous character was detected upon a \\nphotograph of the Martian disk. One needs to see the drawings of these ap- pearances in \\norder to appreciate fully their remark able resemblance in character.  \\nAt any rate, whether we expect another inva sion or not, our views of the human future\\n\\nLessing has advanced excellent reasons fo r supposing that the Martians have actually \\nsucceeded in effecting a landing on the planet  Venus. Seven months ago now, Venus and \\nMars were in alignment with the sun; that  is to say, Mars was in opposition from the \\npoint of view of an observer on Venus. Subsequently a peculiar luminous and sinuous\\n\\nexisted beyond the petty surface of our minut e sphere. Now we see further. If the \\nMartians can reach Venus, there is no reason to suppose that the thing is impossible for \\nmen, and when the slow cooling of the sun make s this earth uninhabitable, as at last it \\nmust do, it may be that the thread of life th at has begun here will have streamed out and \\ncaught our sister planet within its toils.  \\n   Dim and wonderful is the vision I have conj ured up in my mind of  life spreading slowly\\n\\nit has done much to promote th e conception of the commonweal of mankind. It may be \\nthat across the immensity of space the Martians have watched the fate of these pioneers of theirs and learned their lesson, and that on the planet Venus they have found a securer \\nsettlement. Be that as it may, for many year s yet there will certainly be no relaxation of \\nthe eager scrutiny of the Martian disk, and thos e fiery darts of the sky, the shooting stars,\\n\\nQuestion: did the aliens eventually make it to Venus?\\nHelpful Answer:\"\n",
      "Context passed to LLM: \n",
      "Response from LLM: According to the text, it is possible that the Martians have actually succeeded in effecting a landing on Venus. This is based on an alignment of Mars and Venus with the sun, which occurred seven months prior to the writing of the passage. However, there is no definitive answer provided as to whether the aliens did indeed make it to Venus or not. The passage concludes by saying that the Martian disk will continue to be closely scrutinized for many years to come.\n",
      "Memory state: [HumanMessage(content='did the aliens eventually make it to Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content='According to the text, it is possible that the Martians have actually succeeded in effecting a landing on Venus. This is based on an alignment of Mars and Venus with the sun, which occurred seven months prior to the writing of the passage. However, there is no definitive answer provided as to whether the aliens did indeed make it to Venus or not. The passage concludes by saying that the Martian disk will continue to be closely scrutinized for many years to come. Thanks for asking!', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: Where does the novel take place?\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: did the aliens eventually make it to Venus?\\nAssistant: According to the text, it is possible that the Martians have actually succeeded in effecting a landing on Venus. This is based on an alignment of Mars and Venus with the sun, which occurred seven months prior to the writing of the passage. However, there is no definitive answer provided as to whether the aliens did indeed make it to Venus or not. The passage concludes by saying that the Martian disk will continue to be closely scrutinized for many years to come. Thanks for asking!\\nFollow Up Input: Where does the novel take place?\\nStandalone question:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: Based on the provided chat history and follow-up input, I would rephrase the follow-up question as a standalone question in its original language:\n",
      "\n",
      "Where is the novel set?\n",
      "\n",
      "Note that this standalone question corresponds to the original follow-up input \"Where does the novel take place?\".\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\ndescribed the behaviour of the men and women to whom I spoke. A ll over the district \\npeople were dining and supping; working men were gardening after the labours of the \\nday, children were being put to bed, young people were wandering through the lanes \\nlove-making, stu- dents sat over their books.  \\n   Maybe there was a murmur in the village streets, a novel and dominant topic in the \\npublic-houses, and here and there a messenger,  or even an eye-witness of the later\\n\\n\"Where\\'s your shells?\" said the first speaker. \"There ain\\'t no time. Do it in a rush, that\\'s \\nmy tip, and do it at once.\"     So they discussed it. After a while I left them, and went on to th e railway station to get \\nas many morning papers as I could.  \\n    But I will not weary the reader with a description of that long morning and of the \\nlonger afternoon. I did not succeed  in getting a glimpse of th e common, for even Horsell\\n\\nAbout three o\\'clock there began the thud of a gu n at measured intervals from Chertsey or \\nAddlestone. I learned that the smouldering pine wood into which the second cylin- der \\nhad fallen was being shelled, in the hope of destroying that ob ject before it opened. It was \\nonly about five, however, that a field gun r eached Chobham for use against the first body \\nof Martians.     About six in the evening, as I sat at tea with my wife in the summerhouse talking\\n\\nmust have been used for some reason. A nd in the garden beyond Roehampton I got a \\nquan- tity of immature potat oes, sufficient to stay my hunger. From this garden one \\nlooked down upon Putney and the river. The aspect of the place in the dusk was \\nsingularly desolate: blackened trees, blackened, de solate ruins, and down the hill the \\nsheets of the flooded river, red-tinged with th e weed. And over all--sil ence. It filled me\\n\\nQuestion: Based on the provided chat history and follow-up input, I would rephrase the follow-up question as a standalone question in its original language:\\n\\nWhere is the novel set?\\n\\nNote that this standalone question corresponds to the original follow-up input \"Where does the novel take place?\". Thanks for asking!\\nHelpful Answer:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: The context is a passage from H.G. Wells' novel \"The War of the Worlds\", which describes the events and atmosphere in a small village and its surroundings as a Martian invasion begins. The passage sets the scene and provides background information before moving on to describe specific events and actions taken by the protagonist.\n",
      "Memory state: [HumanMessage(content='did the aliens eventually make it to Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content='According to the text, it is possible that the Martians have actually succeeded in effecting a landing on Venus. This is based on an alignment of Mars and Venus with the sun, which occurred seven months prior to the writing of the passage. However, there is no definitive answer provided as to whether the aliens did indeed make it to Venus or not. The passage concludes by saying that the Martian disk will continue to be closely scrutinized for many years to come. Thanks for asking!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Where does the novel take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage from H.G. Wells\\' novel \"The War of the Worlds\", which describes the events and atmosphere in a small village and its surroundings as a Martian invasion begins. The passage sets the scene and provides background information before moving on to describe specific events and actions taken by the protagonist. Thanks for asking!', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: IN what city does the novel take place?\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: did the aliens eventually make it to Venus?\\nAssistant: According to the text, it is possible that the Martians have actually succeeded in effecting a landing on Venus. This is based on an alignment of Mars and Venus with the sun, which occurred seven months prior to the writing of the passage. However, there is no definitive answer provided as to whether the aliens did indeed make it to Venus or not. The passage concludes by saying that the Martian disk will continue to be closely scrutinized for many years to come. Thanks for asking!\\nHuman: Where does the novel take place?\\nAssistant: The context is a passage from H.G. Wells\\' novel \"The War of the Worlds\", which describes the events and atmosphere in a small village and its surroundings as a Martian invasion begins. The passage sets the scene and provides background information before moving on to describe specific events and actions taken by the protagonist. Thanks for asking!\\nFollow Up Input: IN what city does the novel take place?\\nStandalone question:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: Based on the conversation, I would rephrase the follow-up input as a standalone question:\n",
      "\n",
      "\"Where is the setting of H.G. Wells' novel 'The War of the Worlds'?\"\n",
      "\n",
      "This question captures the essence of the original follow-up input (\"IN what city does the novel take place?\"), but in a more natural and concise way.\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nthat one at Weybridge was an  accident. And these are only  pioneers. They kept on \\ncoming. These green stars--I\\'ve seen none these five or six days, but I\\'ve no doubt they\\'re \\nfalling somewhere every night. Nothing\\'s to  be done. We\\'re under! We\\'re beat!\"  \\n   I made him no answer. I sat staring be fore me, trying in vain to devise some \\ncountervailing thought.  \\n   \"This isn\\'t a war,\" said the artilleryman. \"It never was a war, any more than there\\'s war\\n\\nsailors and lightermen had to fight savagely  against the people who swarmed upon them \\nfrom the riverfront. People were actually clambering down the piers of the bridge from \\nabove.  \\n   When, an hour later, a Martian appear ed beyond the Clock Tower and waded down the \\nriver, nothing but wreck- age floated above Limehouse.  \\n   Of the falling of the fift h cylinder I have presently to tell. The sixth star fell at\\n\\ncourse of the evening\\'s fighting. I did not  know even the circumstances that had \\nprecipitated the conflic t. As I came through Ockham (for th at was the way I returned, and \\nnot through Send and Old Woking) I saw al ong the western horizon a blood-red glow, \\nwhich as I drew nearer, crept slowly up th e sky. The driving clouds of the gathering \\nthunder- storm mingled there with masses of black and red smoke.\\n\\nEnglish readers heard of it firs t in the issue of NATURE date d August 2. I am inclined to \\nthink that this blaze may have  been the casting of the huge g un, in the vast pit sunk into \\ntheir planet, from which th eir shots were fired at us . Peculiar markings, as yet \\nunexplained, were seen near the site of that outbreak during the next two oppositions.  \\n   The storm burst upon us six years ago now . As Mars approached opposition, Lavelle of\\n\\nQuestion: Based on the conversation, I would rephrase the follow-up input as a standalone question:\\n\\n\"Where is the setting of H.G. Wells\\' novel \\'The War of the Worlds\\'?\"\\n\\nThis question captures the essence of the original follow-up input (\"IN what city does the novel take place?\"), but in a more natural and concise way. Thanks for asking!\\nHelpful Answer:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: A helpful answer!\n",
      "\n",
      "The context is a passage from H.G. Wells' novel \"The War of the Worlds\", specifically a conversation between the narrator and an artilleryman. The passage describes the aftermath of a Martian invasion and the devastation caused by the fall of Martians' heat-rays (referred to as \"green stars\") on Earth.\n",
      "Memory state: [HumanMessage(content='did the aliens eventually make it to Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content='According to the text, it is possible that the Martians have actually succeeded in effecting a landing on Venus. This is based on an alignment of Mars and Venus with the sun, which occurred seven months prior to the writing of the passage. However, there is no definitive answer provided as to whether the aliens did indeed make it to Venus or not. The passage concludes by saying that the Martian disk will continue to be closely scrutinized for many years to come. Thanks for asking!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Where does the novel take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage from H.G. Wells\\' novel \"The War of the Worlds\", which describes the events and atmosphere in a small village and its surroundings as a Martian invasion begins. The passage sets the scene and provides background information before moving on to describe specific events and actions taken by the protagonist. Thanks for asking!', additional_kwargs={}, response_metadata={}), HumanMessage(content='IN what city does the novel take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='A helpful answer!\\n\\nThe context is a passage from H.G. Wells\\' novel \"The War of the Worlds\", specifically a conversation between the narrator and an artilleryman. The passage describes the aftermath of a Martian invasion and the devastation caused by the fall of Martians\\' heat-rays (referred to as \"green stars\") on Earth. Thanks for asking!', additional_kwargs={}, response_metadata={})]\n",
      "Question passed to retriever: What happens in the novel in London?\n",
      "Question passed to LLM: text='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: did the aliens eventually make it to Venus?\\nAssistant: According to the text, it is possible that the Martians have actually succeeded in effecting a landing on Venus. This is based on an alignment of Mars and Venus with the sun, which occurred seven months prior to the writing of the passage. However, there is no definitive answer provided as to whether the aliens did indeed make it to Venus or not. The passage concludes by saying that the Martian disk will continue to be closely scrutinized for many years to come. Thanks for asking!\\nHuman: Where does the novel take place?\\nAssistant: The context is a passage from H.G. Wells\\' novel \"The War of the Worlds\", which describes the events and atmosphere in a small village and its surroundings as a Martian invasion begins. The passage sets the scene and provides background information before moving on to describe specific events and actions taken by the protagonist. Thanks for asking!\\nHuman: IN what city does the novel take place?\\nAssistant: A helpful answer!\\n\\nThe context is a passage from H.G. Wells\\' novel \"The War of the Worlds\", specifically a conversation between the narrator and an artilleryman. The passage describes the aftermath of a Martian invasion and the devastation caused by the fall of Martians\\' heat-rays (referred to as \"green stars\") on Earth. Thanks for asking!\\nFollow Up Input: What happens in the novel in London?\\nStandalone question:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: What happens in London?\n",
      "Question passed to LLM: text='Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nI go to London and see the busy multitudes in Fleet Street and the Strand, and it comes \\nacross my mind that they are but the ghosts of  the past, haunting the streets that I have \\nseen silent and wretched, going to and fro, phan- tasms in a d ead city, the mockery of life \\nin a galvanised body. And strange, too, it is to stand on Primrose Hill, as I did but a day before writing this last chapter, to see the great province of houses , dim and blue through\\n\\nand behind in the Park Ter- races and in the hundr ed other streets of that part of Maryle- \\nbone, and the Westbourne Park district and St . Pancras, and westwa rd and northward in \\nKilburn and St. John\\'s Wood and Hampstead, and eastward in Shoreditch and Highbury \\nand Haggerston and Hoxton, and, indeed, thr ough all the vastness of  London from Ealing \\nto East Ham--people were r ubbing their eyes, and opening windows to stare out and ask\\n\\nor a grim resolution. Save for the expression of the faces, London seemed a city of tramps. The vestries were i ndiscriminately distributing br ead sent us by the French \\ngovernment. The ribs of the few horses show ed dismally. Haggard special constables \\nwith white badges stood at the corners of ever y street. I saw little of  the mischief wrought \\nby the Martians until I reached Welling- t on Street, and there I saw the red weed \\nclambering over the buttresses of Waterloo Bridge.\\n\\nIn London ...................................................................................................................... 52 \\nWhat Had Happened In Surrey ..................................................................................... 60 \\nThe Exodus From London ............................................................................................ 66 \\nThe \"Thunder Child\" ..................................................................................................... 75\\n\\nQuestion: What happens in London? Thanks for asking!\\nHelpful Answer:'\n",
      "Context passed to LLM: \n",
      "Response from LLM: Based on the provided text, here's what we can gather:\n",
      "\n",
      "The narrator returns to London and is struck by the contrast between the bustling streets they see now and the desolate scene they witnessed earlier during a Martian invasion. They observe that people are rubbing their eyes and opening windows to stare out in shock and amazement.\n",
      "\n",
      "Additionally, the narrator notes that the city seems like it's being inhabited by tramps, with vestries distributing bread sent by the French government. Haggard special constables with white badges are stationed at street corners. The narrator also sees the effects of the Martian invasion, including the \"red weed\" (a type of vegetation) covering Waterloo Bridge.\n",
      "\n",
      "It seems that London is still recovering from the devastating events described earlier in the text, and the city is struggling to come back to life.\n",
      "Memory state: [HumanMessage(content='did the aliens eventually make it to Venus?', additional_kwargs={}, response_metadata={}), AIMessage(content='According to the text, it is possible that the Martians have actually succeeded in effecting a landing on Venus. This is based on an alignment of Mars and Venus with the sun, which occurred seven months prior to the writing of the passage. However, there is no definitive answer provided as to whether the aliens did indeed make it to Venus or not. The passage concludes by saying that the Martian disk will continue to be closely scrutinized for many years to come. Thanks for asking!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Where does the novel take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='The context is a passage from H.G. Wells\\' novel \"The War of the Worlds\", which describes the events and atmosphere in a small village and its surroundings as a Martian invasion begins. The passage sets the scene and provides background information before moving on to describe specific events and actions taken by the protagonist. Thanks for asking!', additional_kwargs={}, response_metadata={}), HumanMessage(content='IN what city does the novel take place?', additional_kwargs={}, response_metadata={}), AIMessage(content='A helpful answer!\\n\\nThe context is a passage from H.G. Wells\\' novel \"The War of the Worlds\", specifically a conversation between the narrator and an artilleryman. The passage describes the aftermath of a Martian invasion and the devastation caused by the fall of Martians\\' heat-rays (referred to as \"green stars\") on Earth. Thanks for asking!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What happens in the novel in London?', additional_kwargs={}, response_metadata={}), AIMessage(content='Based on the provided text, here\\'s what we can gather:\\n\\nThe narrator returns to London and is struck by the contrast between the bustling streets they see now and the desolate scene they witnessed earlier during a Martian invasion. They observe that people are rubbing their eyes and opening windows to stare out in shock and amazement.\\n\\nAdditionally, the narrator notes that the city seems like it\\'s being inhabited by tramps, with vestries distributing bread sent by the French government. Haggard special constables with white badges are stationed at street corners. The narrator also sees the effects of the Martian invasion, including the \"red weed\" (a type of vegetation) covering Waterloo Bridge.\\n\\nIt seems that London is still recovering from the devastating events described earlier in the text, and the city is struggling to come back to life. Thanks for asking!', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/968501674.py\", line 107, in get_important_facts\n",
      "    return get_current_weather(city)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/968501674.py\", line 35, in get_current_weather\n",
      "    raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
      "Exception: Weather API Request failed with status code: 401\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/968501674.py\", line 107, in get_important_facts\n",
      "    return get_current_weather(city)\n",
      "  File \"/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_33424/968501674.py\", line 35, in get_current_weather\n",
      "    raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
      "Exception: Weather API Request failed with status code: 401\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.runnables import Runnable\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Weather tool function\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"Fetch current weather for a given city.\"\"\"\n",
    "    \n",
    "    API_KEY = 'YOUR_OPENWEATHERMAP_API_KEY'  # Replace with your OpenWeatherMap API key\n",
    "    BASE_URL = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    \n",
    "    params = {\n",
    "        'q': city,\n",
    "        'appid': API_KEY,\n",
    "        'units': 'metric',  # To get the temperature in Celsius\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        temperature = results['main']['temp']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        humidity = results['main']['humidity']\n",
    "        wind_speed = results['wind']['speed']\n",
    "        pressure = results['main']['pressure']\n",
    "    else:\n",
    "        raise Exception(f\"Weather API Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    return (\n",
    "        f\"The current temperature in {city} is {temperature}°C with {weather_description}.\\n\"\n",
    "        f\"Humidity: {humidity}%, Wind Speed: {wind_speed} m/s, Pressure: {pressure} hPa.\\n\"\n",
    "        f\"Thanks for asking!\"\n",
    "    )\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define memory to store the previous exchanges\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Custom Runnable LLM class with debugging\n",
    "class OllamaLLM(Runnable):\n",
    "    def __init__(self, llm_fn):\n",
    "        self.llm_fn = llm_fn\n",
    "\n",
    "    def invoke(self, input, config=None, **kwargs):\n",
    "        question = str(input)\n",
    "        context = kwargs.get(\"context\", \"\")\n",
    "        print(f\"Question passed to LLM: {question}\")\n",
    "        print(f\"Context passed to LLM: {context}\")\n",
    "        stop = kwargs.get(\"stop\", None)\n",
    "        response = self.llm_fn(question, context)\n",
    "        print(f\"Response from LLM: {response}\")\n",
    "        return response + \" Thanks for asking!\"\n",
    "\n",
    "    def predict(self, input, **kwargs):\n",
    "        return self.invoke(input, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.invoke(*args, **kwargs)\n",
    "\n",
    "# Instantiate the custom LLM class\n",
    "ollama_llm_instance = OllamaLLM(ollama_llm)\n",
    "\n",
    "# Define the conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ollama_llm_instance,\n",
    "    retriever=retriever,\n",
    "    memory=memory  # add the memory module to pass the previous exchange to the LLM as well\n",
    ")\n",
    "\n",
    "# Function to detect if the question is about the weather in a city\n",
    "def is_weather_question(question: str) -> bool:\n",
    "    return \"weather\" in question.lower() and \"in\" in question.lower()\n",
    "\n",
    "# Extract city name from the weather question\n",
    "def extract_city_from_question(question: str) -> str:\n",
    "    # Simple heuristic to extract city name\n",
    "    if \"weather in\" in question.lower():\n",
    "        return question.lower().split(\"weather in\")[1].strip().split()[0].capitalize()\n",
    "    return \"\"\n",
    "\n",
    "# Define the function to get important facts with debugging\n",
    "def get_important_facts(question):\n",
    "    # Check if the question is about the weather in a city\n",
    "    if is_weather_question(question):\n",
    "        city = extract_city_from_question(question)\n",
    "        if city:\n",
    "            return get_current_weather(city)\n",
    "        else:\n",
    "            return \"I couldn't determine the city you're asking about. Please specify the city.\"\n",
    "    \n",
    "    # Otherwise, use the LLM-based chain to answer the question directly\n",
    "    print(f\"Question passed to retriever: {question}\")\n",
    "    response = qa_chain.run({\"question\": question})\n",
    "    print(f\"Memory state: {memory.buffer}\")\n",
    "    return response\n",
    "\n",
    "# Create a Gradio app interface\n",
    "iface = gr.Interface(\n",
    "    fn=get_important_facts,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"RAG with Llama3\",\n",
    "    description=\"Ask questions about the provided context\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88def89-4e2a-4cde-8758-0bad7e574ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4620ef7-6b35-4d75-a295-cbe5731fe535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
